{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandiab/PINNs_BuckleyLeverett/blob/main/BuckleyLeverett_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjvyrMzvyxWK"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvXIzacA_Qxy"
      },
      "outputs": [],
      "source": [
        "# Installing Packages \n",
        "\n",
        "!sudo apt-get install dvipng texlive-latex-extra texlive-fonts-recommended \n",
        "!sudo apt-get install dvipng texlive-latex-extra texlive-fonts-recommended cm-super \n",
        "!pip install pyDOE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKypJ49Y2WoH"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import sys\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow.experimental.numpy as tnp\n",
        "import tensorflow_probability as tfp\n",
        "import scipy\n",
        "from scipy import io\n",
        "import random\n",
        "import scipy.io\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pyDOE import lhs\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "from scipy.interpolate import griddata\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib as mpl\n",
        "\n",
        "# Manually making sure the numpy random seeds are \"the same\" on all devices\n",
        "random.seed(1234)\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_LE2a2Ry3e9"
      },
      "source": [
        "Plotting and Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnpT7dmY7vSF"
      },
      "outputs": [],
      "source": [
        "mpl.use('pgf')\n",
        "\n",
        "def figsize(scale, nplots = 1):\n",
        "    fig_width_pt = 690.0                          # Get this from LaTeX using \\the\\textwidth\n",
        "    inches_per_pt = 1.0/72.27                       # Convert pt to inch\n",
        "    golden_mean = (np.sqrt(5.0)-1.0)/2.0            # Aesthetic ratio (you could change this)\n",
        "    fig_width = fig_width_pt*inches_per_pt*scale    # width in inches\n",
        "    fig_height = nplots*fig_width*golden_mean              # height in inches\n",
        "    fig_size = [fig_width,fig_height]\n",
        "    return fig_size\n",
        "\n",
        "pgf_with_latex = {                      # setup matplotlib to use latex for output\n",
        "    \"pgf.texsystem\": \"pdflatex\",        # change this if using xetex or lautex\n",
        "    \"text.usetex\": True,                # use LaTeX to write all text\n",
        "    \"font.family\": \"serif\",\n",
        "    \"font.serif\": [],                   # blank entries should cause plots to inherit fonts from the document\n",
        "    \"font.sans-serif\": [],\n",
        "    \"font.monospace\": [],\n",
        "    \"axes.labelsize\": 8,               # LaTeX default is 10pt font.\n",
        "    \"font.size\": 8,\n",
        "    \"legend.fontsize\": 8,               # Make the legend/label fonts a little smaller\n",
        "    \"xtick.labelsize\": 6,\n",
        "    \"ytick.labelsize\": 6,\n",
        "    \"figure.figsize\": figsize(1.0),     # default fig size of 0.9 textwidth\n",
        "    \"pgf.preamble\": [\n",
        "        r\"\\usepackage[utf8x]{inputenc}\",    # use utf8 fonts becasue your computer can handle it :)\n",
        "        r\"\\usepackage[T1]{fontenc}\",        # plots will be generated using this preamble\n",
        "        ]\n",
        "    }\n",
        "mpl.rcParams.update(pgf_with_latex)\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# I make my own newfig and savefig functions\n",
        "def newfig(width, nplots = 1):\n",
        "    fig = plt.figure(figsize=figsize(width, nplots), dpi=300)\n",
        "    ax = fig.add_subplot(111)\n",
        "    return fig, ax\n",
        "\n",
        "def plot_inf_cont_results(X_star, u_pred, X_u_train, u_train, Exact_u, X, T, x, t):\n",
        "\n",
        "  # Interpolating the results on the whole (x,t) domain.\n",
        "  # griddata(points, values, points at which to interpolate, method)\n",
        "  U_pred = griddata(X_star, u_pred, (X, T), method='cubic')\n",
        "\n",
        "  # Creating the figures\n",
        "  fig, ax = newfig(1.0, 1.1)\n",
        "  ax.axis('off')\n",
        "\n",
        "  ####### Row 0: u(t,x) ##################    \n",
        "  gs0 = gridspec.GridSpec(1, 2)\n",
        "  gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
        "  ax = plt.subplot(gs0[:, :])\n",
        "\n",
        "  h = ax.imshow(U_pred.T, interpolation='nearest', cmap='viridis', \n",
        "                extent=[t.min(), t.max(), x.min(), x.max()], \n",
        "                origin='lower', aspect='auto')\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "  fig.colorbar(h, cax=cax)\n",
        "\n",
        "  ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
        "\n",
        "  line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "  ax.plot(t[10]*np.ones((2,1)), line, 'r--', linewidth = 1)\n",
        "  ax.plot(t[25]*np.ones((2,1)), line, 'r--', linewidth = 1)\n",
        "  ax.plot(t[40]*np.ones((2,1)), line, 'r--', linewidth = 1)\n",
        "  ax.plot(t[55]*np.ones((2,1)), line, 'r--', linewidth = 1)\n",
        "  ax.plot(t[70]*np.ones((2,1)), line, 'r--', linewidth = 1)\n",
        "  ax.plot(t[90]*np.ones((2,1)), line, 'r--', linewidth = 1)    \n",
        "\n",
        "  ax.set_xlabel('$t$')\n",
        "  ax.set_ylabel('$x$')\n",
        "  ax.legend(frameon=False, loc = 'best')\n",
        "  ax.set_title('$u(t,x)$', fontsize = 5)\n",
        "\n",
        "  ####### Row 1: u(t,x) slices ##################    \n",
        "  gs1 = gridspec.GridSpec(2, 5)\n",
        "  gs1.update(top=1-1/4, bottom=0.2, left=0.1, right=0.9, wspace=0.5)\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 0])\n",
        "  ax.plot(x,Exact_u[10,:], color='blue', linewidth = 1.5, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[10,:], 'r--', linewidth = 1.5, label = 'Prediction')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')    \n",
        "  ax.set_title('$t = 0.10$', fontsize = 7)\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-0.1,1.1])\n",
        "  ax.set_ylim([-0.1,1.1])\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 1])\n",
        "  ax.plot(x,Exact_u[25,:], color='blue', linewidth = 1.5, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[25,:], 'r--', linewidth = 1.5, label = 'Prediction')  \n",
        "  ax.set_title('$t = 0.25$', fontsize = 7)\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-0.1,1.1])\n",
        "  ax.set_ylim([-0.1,1.1])\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 2])\n",
        "  ax.plot(x,Exact_u[40,:], color='blue', linewidth = 1.5, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[40,:], 'r--', linewidth = 1.5, label = 'Prediction')  \n",
        "  ax.set_title('$t = 0.40$', fontsize = 7)\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-0.1,1.1])\n",
        "  ax.set_ylim([-0.1,1.1])\n",
        "\n",
        "  ax = plt.subplot(gs1[1, 0])\n",
        "  ax.plot(x,Exact_u[55,:], color='blue', linewidth = 1.5, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[55,:], 'r--', linewidth = 1.5, label = 'Prediction')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-0.1,1.1])\n",
        "  ax.set_ylim([-0.1,1.1])\n",
        "  ax.set_title('$t = 0.55$', fontsize = 7)\n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "\n",
        "  ax = plt.subplot(gs1[1, 1])\n",
        "  ax.plot(x,Exact_u[70,:], color='blue', linewidth = 1.5, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[70,:], 'r--', linewidth = 1.5, label = 'Prediction')   \n",
        "  ax.set_title('$t = 0.70$', fontsize = 7)\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-0.1,1.1])\n",
        "  ax.set_ylim([-0.1,1.1])\n",
        "\n",
        "  ax = plt.subplot(gs1[1, 2])\n",
        "  ax.plot(x,Exact_u[90,:], color='blue', linewidth = 1.5, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[90,:], 'r--', linewidth = 1.5, label = 'Prediction')\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-0.1,1.1])\n",
        "  ax.set_ylim([-0.1,1.1])    \n",
        "  ax.set_title('$t = 0.90$', fontsize = 7)\n",
        "\n",
        "  plt.show();\n",
        "\n",
        "class Logger(object):\n",
        "  def __init__(self, frequency=20):\n",
        "    print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "    print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "    print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
        "\n",
        "    self.start_time = time.time()\n",
        "    self.frequency = frequency\n",
        "\n",
        "  def __get_elapsed(self):\n",
        "    return datetime.fromtimestamp(time.time() - self.start_time).strftime(\"%M:%S\")\n",
        "\n",
        "  def __get_error_u(self):\n",
        "    return self.error_fn()\n",
        "\n",
        "  def set_error_fn(self, error_fn):\n",
        "    self.error_fn = error_fn\n",
        "  \n",
        "  def log_train_start(self, model):\n",
        "    print(\"\\nTraining started\")\n",
        "    print(\"================\")\n",
        "\n",
        "  def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
        "    if epoch % self.frequency == 0:\n",
        "      print(f\"{'nt_epoch' if is_iter else 'tf_epoch'} = {epoch:6d}  elapsed = {self.__get_elapsed()}  loss = {loss:.4e} \" + custom)\n",
        "\n",
        "  def log_train_opt(self, name):\n",
        "    print(f\"—— Starting {name} optimization ——\")\n",
        "\n",
        "  def log_train_end(self, epoch, custom=\"\"):\n",
        "    print(\"==================\")\n",
        "    print(f\"Training finished (epoch {epoch}): duration = {self.__get_elapsed()}  error = {self.__get_error_u():.4e}  \" + custom)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prep Data"
      ],
      "metadata": {
        "id": "ohTlfkNIAnIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_data(path, N_u=None, N_f=None):\n",
        "    # Reading external data [t is 100x1, usol is 256x100 (solution), x is 256x1]\n",
        "    #data = scipy.io.loadmat(path)\n",
        "    data = path\n",
        "    # Flatten makes [[]] into [], [:,None] makes it a column vector\n",
        "    t = data['t'].flatten()[:,None] # T x 1\n",
        "    x = data['x'].flatten()[:,None] # N x 1\n",
        "\n",
        "    # Keeping the 2D data for the solution data (real() is maybe to make it float by default, in case of zeroes)\n",
        "    Exact_u = np.real(data['usol']).T # T x N\n",
        "\n",
        "    # Meshing x and t in 2D (256,100)\n",
        "    X, T = np.meshgrid(x,t)\n",
        "\n",
        "    # Preparing the inputs x and t (meshed as X, T) for predictions in one single array, as X_star\n",
        "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "\n",
        "    # Preparing the testing u_star\n",
        "    u_star = Exact_u.flatten()[:,None]\n",
        "                \n",
        "    # Noiseless data TODO: add support for noisy data    \n",
        "    idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
        "    X_u_train = X_star[idx,:]\n",
        "    u_train = u_star[idx,:]\n",
        "\n",
        "    # Domain bounds (lowerbounds upperbounds) [x, t], which are here ([-1.0, 0.0] and [1.0, 1.0])\n",
        "    lb = X_star.min(axis=0)\n",
        "    ub = X_star.max(axis=0) \n",
        "    # Getting the initial conditions (t=0)\n",
        "    xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
        "    uu1 = Exact_u[0:1,:].T\n",
        "    # Getting the lowest boundary conditions (x=0) \n",
        "    xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
        "    uu2 = Exact_u[:,0:1]\n",
        "\n",
        "    # Stacking them in multidimensional tensors for training (X_u_train is for now the continuous boundaries)\n",
        "    X_u_train = np.vstack([xx1, xx2])\n",
        "    u_train = np.vstack([uu1, uu2])\n",
        "\n",
        "    # Generating the x and t collocation points for f, with each having a N_f size\n",
        "    # We pointwise add and multiply to spread the LHS over the 2D domain\n",
        "    X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
        "\n",
        "    # Generating a uniform random sample from ints between 0, and the size of x_u_train, of size N_u (initial data size) and without replacement (unique)\n",
        "    idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
        "    # Getting the corresponding X_u_train (which is now scarce boundary/initial coordinates)\n",
        "    X_u_train = X_u_train[idx,:]\n",
        "    # Getting the corresponding u_train\n",
        "    u_train = u_train [idx,:]\n",
        "\n",
        "    return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, X_f_train, ub, lb"
      ],
      "metadata": {
        "id": "O3ULYCebAmP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue5Wz7Dl2rBo"
      },
      "source": [
        "# HYPER PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFnT-9oS2cDM"
      },
      "outputs": [],
      "source": [
        "# Data size on the solution u\n",
        "N_u = 300\n",
        "# Collocation points size, where we’ll check for f = 0\n",
        "N_f = 10000\n",
        "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
        "layers = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
        "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
        "tf_epochs = 40000\n",
        "tf_optimizer = tf.keras.optimizers.Adam(\n",
        "  learning_rate= 0.011,\n",
        "  beta_1=0.9,\n",
        "  epsilon=1e-1)\n",
        "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
        "nt_epochs = 1 #This number is not important as long as it is not 0, as LBGFS optimizer will run until convergence "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANj1BCHoyoA9"
      },
      "source": [
        "# Physics Informed NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5htyewH9NLa"
      },
      "outputs": [],
      "source": [
        "class PhysicsInformedNN(object):\n",
        "  def __init__(self, layers, optimizer, logger, X_f, ub, lb):\n",
        "\n",
        "    # Descriptive Keras model [2, 20, …, 20, 1]\n",
        "    self.u_model = tf.keras.Sequential()\n",
        "    self.u_model.add(tf.keras.layers.InputLayer(input_shape=(layers[0],)))\n",
        "    self.u_model.add(tf.keras.layers.Lambda(\n",
        "      lambda X: 1.0*(X - lb)/(ub - lb) - 0.0))\n",
        "    for width in layers[1:]:\n",
        "        self.u_model.add(tf.keras.layers.Dense(\n",
        "          width, activation=tf.nn.tanh,\n",
        "          kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Computing the sizes of weights/biases for future decomposition\n",
        "    self.sizes_w = []\n",
        "    self.sizes_b = []\n",
        "    for i, width in enumerate(layers):\n",
        "      if i != 1:\n",
        "        self.sizes_w.append(int(width * layers[1]))\n",
        "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.logger = logger\n",
        "\n",
        "    self.dtype = tf.float32\n",
        "\n",
        "    # Separating the collocation coordinates\n",
        "    self.x_f = tf.convert_to_tensor(X_f[:, 0:1], dtype=self.dtype)\n",
        "    self.t_f = tf.convert_to_tensor(X_f[:, 1:2], dtype=self.dtype)\n",
        "    \n",
        "  # Defining custom loss\n",
        "  @tf.function\n",
        "  def __loss(self, u, u_pred):\n",
        "    f_pred, weight_resd1 = self.f_model()\n",
        "    return tf.reduce_mean(tf.square(u - u_pred)) + tf.reduce_mean(tf.square(f_pred))\n",
        "\n",
        "  @tf.function\n",
        "  def __grad(self, X, u):\n",
        "    with tf.GradientTape() as tape:\n",
        "      loss_value = self.__loss(u, self.u_model(X))\n",
        "    return loss_value, tape.gradient(loss_value, self.__wrap_training_variables())\n",
        "\n",
        "  def __wrap_training_variables(self):\n",
        "    var = self.u_model.trainable_variables\n",
        "    return var\n",
        "\n",
        "  # The actual PINN\n",
        "  @tf.function\n",
        "  def f_model(self):\n",
        "    #here u is the S (saturation function)\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      tape.watch(self.x_f)\n",
        "      tape.watch(self.t_f)\n",
        "      X_f = tf.stack([self.x_f[:,0], self.t_f[:,0]], axis=1)\n",
        "      u = self.u_model(X_f)\n",
        "\n",
        "      Swc = 0.0\n",
        "      M = 2\n",
        "      Sor = 0.0\n",
        "\n",
        "      #Non-Convex Flux Function\n",
        "      frac_org = tf.divide(tf.square(u-Swc), tf.square(u-Swc) + tf.divide(tf.square(1 - u - Sor), M))\n",
        "      Sf = tf.sqrt(tf.divide(1/M, (1/M)+1))\n",
        "      frac_Sf = tf.divide(tf.square(Sf-Swc), tf.square(Sf-Swc) + tf.divide(tf.square(1 - Sf - Sor), M))\n",
        "      frac = tf.divide(frac_Sf, Sf)*u - tf.multiply(tf.divide(frac_Sf, Sf)*u, tnp.heaviside(u-Sf, 1)) + tf.multiply(frac_org, tnp.heaviside(u-Sf, 1))\n",
        "\n",
        "    u_x = tape.gradient(u, self.x_f)\n",
        "    u_t = tape.gradient(u, self.t_f)\n",
        "    frac_u = tape.gradient(frac, u)\n",
        "    weight_resd = tf.divide(1, (tf.square(u_t) + tf.square(u_x) + 1)) \n",
        "\n",
        "    del tape \n",
        "    \n",
        "    return u_t + frac_u * u_x, weight_resd\n",
        "\n",
        "  def get_weights(self):\n",
        "    w = []\n",
        "    for layer in self.u_model.layers[1:]:\n",
        "      weights_biases = layer.get_weights()\n",
        "      weights = weights_biases[0].flatten()\n",
        "      biases = weights_biases[1]\n",
        "      w.extend(weights)\n",
        "      w.extend(biases)\n",
        "    return tf.convert_to_tensor(w, dtype=self.dtype)\n",
        "\n",
        "  def set_weights(self, w):\n",
        "    for i, layer in enumerate(self.u_model.layers[1:]):\n",
        "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "      weights = w[start_weights:end_weights]\n",
        "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "      weights = tf.reshape(weights, [w_div, self.sizes_b[i]])\n",
        "      biases = w[end_weights:end_weights + self.sizes_b[i]]\n",
        "      weights_biases = [weights, biases]\n",
        "      layer.set_weights(weights_biases)\n",
        "\n",
        "  # The training function\n",
        "  def fit(self, X_u, u, tf_epochs):\n",
        "    self.logger.log_train_start(self)\n",
        "\n",
        "    # Creating the tensors\n",
        "    X_u = tf.convert_to_tensor(X_u, dtype=self.dtype)\n",
        "    u = tf.convert_to_tensor(u, dtype=self.dtype)\n",
        "\n",
        "    self.logger.log_train_opt(\"Adam\")\n",
        "    for epoch in range(tf_epochs):\n",
        "      # Optimization step\n",
        "      loss_value, grads = self.__grad(X_u, u)\n",
        "      self.optimizer.apply_gradients(zip(grads, self.__wrap_training_variables()))\n",
        "      self.logger.log_train_epoch(epoch, loss_value)\n",
        "\n",
        "    self.logger.log_train_opt(\"LBFGS\")\n",
        "    def loss_and_flat_grad(w):\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.set_weights(w)\n",
        "        loss_value = self.__loss(u, self.u_model(X_u))\n",
        "      grad = tape.gradient(loss_value, self.u_model.trainable_variables)\n",
        "      grad_flat = []\n",
        "      for g in grad:\n",
        "        grad_flat.append(tf.reshape(g, [-1]))\n",
        "      grad_flat =  tf.concat(grad_flat, 0)\n",
        "\n",
        "      return loss_value, grad_flat\n",
        "    \n",
        "    tfp.optimizer.lbfgs_minimize(\n",
        "      loss_and_flat_grad,\n",
        "      initial_position=self.get_weights(),\n",
        "      num_correction_pairs=100,\n",
        "      max_iterations=50000,\n",
        "      f_relative_tolerance= 1.0e-16*np.finfo(float).eps,\n",
        "      tolerance= 1.0e-16*np.finfo(float).eps,\n",
        "      parallel_iterations=100)\n",
        "    \n",
        "    self.logger.log_train_end(tf_epochs + 1)\n",
        "\n",
        "  def predict(self, X_star):\n",
        "    u_star = self.u_model(X_star)\n",
        "    return u_star\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIZNI8nc-9qv"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8e4VOHq2hF7"
      },
      "outputs": [],
      "source": [
        "# Getting the data\n",
        "path = scipy.io.loadmat('/content/Buckley_Swc_0_Sor_0_M_2.mat');\n",
        "x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, X_f, ub, lb = prep_data(path, N_u, N_f)\n",
        "\n",
        "# Creating the model and training\n",
        "logger = Logger(frequency=200)\n",
        "pinn = PhysicsInformedNN(layers, tf_optimizer, logger, X_f, ub, lb)\n",
        "def error():\n",
        "  u_pred = pinn.predict(X_star)\n",
        "  return np.linalg.norm(u_star - u_pred) / np.linalg.norm(u_star)\n",
        "logger.set_error_fn(error)\n",
        "\n",
        "pinn.fit(X_u_train, u_train, tf_epochs)\n",
        "\n",
        "# Getting the model predictions, from the same (x,t) that the predictions were previously gotten from\n",
        "u_pred = pinn.predict(X_star)\n",
        "\n",
        "#Plotting\n",
        "plot_inf_cont_results(X_star, u_pred.numpy().flatten(), X_u_train, u_train, Exact_u, X, T, x, t)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sjvyrMzvyxWK",
        "ohTlfkNIAnIH",
        "Ue5Wz7Dl2rBo",
        "ANj1BCHoyoA9"
      ],
      "provenance": [],
      "machine_shape": "hm",
      "mount_file_id": "1SoOIDt-dQSeRxKlYv-i3yJgc1HFBjPq0",
      "authorship_tag": "ABX9TyO+C4jYrPmZtdcEbWAOXkwh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}