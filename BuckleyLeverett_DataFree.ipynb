{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandiab/PINNs_BuckleyLeverett/blob/main/BuckleyLeverett_DataFree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjvyrMzvyxWK"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvXIzacA_Qxy"
      },
      "outputs": [],
      "source": [
        "!pip install pyDOE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "tKypJ49Y2WoH"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import json\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tensorflow.experimental.numpy as tnp\n",
        "import tensorflow_probability as tfp\n",
        "import scipy\n",
        "from scipy import io\n",
        "import random\n",
        "import scipy.io\n",
        "import time\n",
        "from datetime import datetime\n",
        "from pyDOE import lhs\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits import mplot3d\n",
        "from scipy.interpolate import griddata\n",
        "import matplotlib.gridspec as gridspec\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "random.seed(1234)\n",
        "# Manually making sure the numpy random seeds are \"the same\" on all devices\n",
        "np.random.seed(1234)\n",
        "tf.random.set_seed(1234)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_LE2a2Ry3e9"
      },
      "source": [
        "Plotting & Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bnpT7dmY7vSF"
      },
      "outputs": [],
      "source": [
        "\n",
        "mpl.use('pgf')\n",
        "\n",
        "def figsize(scale, nplots = 1):\n",
        "    fig_width_pt = 690.0                          # Get this from LaTeX using \\the\\textwidth\n",
        "    inches_per_pt = 1.0/72.27                       # Convert pt to inch\n",
        "    golden_mean = (np.sqrt(5.0)-1.0)/2.0            # Aesthetic ratio (you could change this)\n",
        "    fig_width = fig_width_pt*inches_per_pt*scale    # width in inches\n",
        "    fig_height = nplots*fig_width*golden_mean              # height in inches\n",
        "    fig_size = [fig_width,fig_height]\n",
        "    return fig_size\n",
        "\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "\n",
        "# I make my own newfig and savefig functions\n",
        "def newfig(width, nplots = 1):\n",
        "    fig = plt.figure(figsize=figsize(width, nplots), dpi=200)\n",
        "    ax = fig.add_subplot(111)\n",
        "    return fig, ax\n",
        "\n",
        "def plot_inf_cont_results(X_star, u_pred, X_u_train, u_train, Exact_u, X, T, x, t):\n",
        "\n",
        "  # Interpolating the results on the whole (x,t) domain.\n",
        "  # griddata(points, values, points at which to interpolate, method)\n",
        "  U_pred = griddata(X_star, u_pred, (X, T), method='cubic')\n",
        "\n",
        "  # Creating the figures\n",
        "  fig, ax = newfig(1.0, 1.1)\n",
        "  ax.axis('off')\n",
        "\n",
        "  ####### Row 0: u(t,x) ##################    \n",
        "  gs0 = gridspec.GridSpec(1, 1)\n",
        "  gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
        "  ax = plt.subplot(gs0[:, :])\n",
        "\n",
        "  h = ax.imshow(U_pred.T, interpolation='nearest', cmap='viridis', \n",
        "                extent=[t.min(), t.max(), x.min(), x.max()], \n",
        "                origin='lower', aspect='auto')\n",
        "  divider = make_axes_locatable(ax)\n",
        "  cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
        "  fig.colorbar(h, cax=cax)\n",
        "\n",
        "  ax.plot(X_u_train[:,1], X_u_train[:,0], 'kx', label = 'Data (%d points)' % (u_train.shape[0]), markersize = 4, clip_on = False)\n",
        "\n",
        "  line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
        "  ax.plot(t[10]*np.ones((2,1)), line, 'r--', linewidth = 1)\n",
        "  ax.plot(t[25]*np.ones((2,1)), line, 'r--', linewidth = 1)\n",
        "  ax.plot(t[40]*np.ones((2,1)), line, 'r--', linewidth = 1)\n",
        "  ax.plot(t[55]*np.ones((2,1)), line, 'r--', linewidth = 1)\n",
        "  ax.plot(t[70]*np.ones((2,1)), line, 'r--', linewidth = 1)\n",
        "  ax.plot(t[90]*np.ones((2,1)), line, 'r--', linewidth = 1)    \n",
        "\n",
        "  ax.set_xlabel('$t$')\n",
        "  ax.set_ylabel('$x$')\n",
        "  ax.legend(frameon=False, loc = 'best')\n",
        "  ax.set_title('$u(t,x)$', fontsize = 5)\n",
        "\n",
        "  ####### Row 1: u(t,x) slices ##################    \n",
        "  gs1 = gridspec.GridSpec(2, 5)\n",
        "  # Change top=1-1/4 to top=1-1/3 to get both plots at the same time \n",
        "  gs1.update(top=1-1/4, bottom=0.2, left=0.1, right=0.9, wspace=0.5)\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 0])\n",
        "  ax.plot(x,Exact_u[10,:], color='blue', linewidth = 1.5, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[10,:], 'r--', linewidth = 1.5, label = 'Prediction')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')    \n",
        "  ax.set_title('$t = 0.10$', fontsize = 7)\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-0.1,1.1])\n",
        "  ax.set_ylim([-0.1,1.1])\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 1])\n",
        "  ax.plot(x,Exact_u[25,:], color='blue', linewidth = 1.5, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[25,:], 'r--', linewidth = 1.5, label = 'Prediction')\n",
        "  ax.set_title('$t = 0.25$', fontsize = 7)\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-0.1,1.1])\n",
        "  ax.set_ylim([-0.1,1.1])\n",
        "\n",
        "  ax = plt.subplot(gs1[0, 2])\n",
        "  ax.plot(x,Exact_u[40,:], color='blue', linewidth = 1.5, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[40,:], 'r--', linewidth = 1.5, label = 'Prediction')\n",
        "  ax.set_title('$t = 0.40$', fontsize = 7)\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-0.1,1.1])\n",
        "  ax.set_ylim([-0.1,1.1])\n",
        "\n",
        "  ax = plt.subplot(gs1[1, 0])\n",
        "  ax.plot(x,Exact_u[55,:], color='blue', linewidth = 1.5, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[55,:], 'r--', linewidth = 1.5, label = 'Prediction')\n",
        "  ax.set_xlabel('$x$')\n",
        "  ax.set_ylabel('$u(t,x)$')\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-0.1,1.1])\n",
        "  ax.set_ylim([-0.1,1.1])\n",
        "  ax.set_title('$t = 0.55$', fontsize = 7)\n",
        "  ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.35), ncol=5, frameon=False)\n",
        "\n",
        "  ax = plt.subplot(gs1[1, 1])\n",
        "  ax.plot(x,Exact_u[70,:], color='blue', linewidth = 1.5, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[70,:], 'r--', linewidth = 1.5, label = 'Prediction')  \n",
        "  ax.set_title('$t = 0.70$', fontsize = 7)\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-0.1,1.1])\n",
        "  ax.set_ylim([-0.1,1.1])\n",
        "\n",
        "  ax = plt.subplot(gs1[1, 2])\n",
        "  ax.plot(x,Exact_u[90,:], color='blue', linewidth = 1.5, label = 'Exact')       \n",
        "  ax.plot(x,U_pred[90,:], 'r--', linewidth = 1.5, label = 'Prediction')\n",
        "  ax.axis('square')\n",
        "  ax.set_xlim([-0.1,1.1])\n",
        "  ax.set_ylim([-0.1,1.1])    \n",
        "  ax.set_title('$t = 0.90$', fontsize = 7)\n",
        "\n",
        "  plt.show();\n",
        "\n",
        "class Logger(object):\n",
        "  def __init__(self, frequency=20):\n",
        "    print(\"TensorFlow version: {}\".format(tf.__version__))\n",
        "    print(\"Eager execution: {}\".format(tf.executing_eagerly()))\n",
        "    print(\"GPU-accerelated: {}\".format(tf.test.is_gpu_available()))\n",
        "\n",
        "    self.start_time = time.time()\n",
        "    self.frequency = frequency\n",
        "\n",
        "  def __get_elapsed(self):\n",
        "    return datetime.fromtimestamp(time.time() - self.start_time).strftime(\"%M:%S\")\n",
        "\n",
        "  def __get_error_u(self):\n",
        "    return self.error_fn()\n",
        "\n",
        "  def set_error_fn(self, error_fn):\n",
        "    self.error_fn = error_fn\n",
        "  \n",
        "  def log_train_start(self, model):\n",
        "    print(\"\\nTraining started\")\n",
        "    print(\"================\")\n",
        "    #self.model = model\n",
        "    #print(self.model.summary())\n",
        "\n",
        "  def log_train_epoch(self, epoch, loss, custom=\"\", is_iter=False):\n",
        "    if epoch % self.frequency == 0:\n",
        "      print(f\"{'nt_epoch' if is_iter else 'tf_epoch'} = {epoch:6d}  elapsed = {self.__get_elapsed()}  loss = {loss:.4e} \" + custom)\n",
        "\n",
        "  def log_train_opt(self, name):\n",
        "    # print(f\"tf_epoch =      0  elapsed = 00:00  loss = 2.7391e-01  error = 9.0843e-01\")\n",
        "    print(f\"—— Starting {name} optimization ——\")\n",
        "\n",
        "  def log_train_end(self, epoch, custom=\"\"):\n",
        "    print(\"==================\")\n",
        "    print(f\"Training finished (epoch {epoch}): duration = {self.__get_elapsed()}\" + custom)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2MxPMxZy9ni"
      },
      "source": [
        "custom_lbfgs.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AZCIox4Y8Ve1"
      },
      "outputs": [],
      "source": [
        "# Adapted from https://github.com/yaroslavvb/stuff/blob/master/eager_lbfgs/eager_lbfgs.py\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# Time tracking functions\n",
        "global_time_list = []\n",
        "global_last_time = 0\n",
        "def reset_time():\n",
        "  global global_time_list, global_last_time\n",
        "  global_time_list = []\n",
        "  global_last_time = time.perf_counter()\n",
        "  \n",
        "def record_time():\n",
        "  global global_last_time, global_time_list\n",
        "  new_time = time.perf_counter()\n",
        "  global_time_list.append(new_time - global_last_time)\n",
        "  global_last_time = time.perf_counter()\n",
        "  #print(\"step: %.2f\"%(global_time_list[-1]*1000))\n",
        "\n",
        "def last_time():\n",
        "  \"\"\"Returns last interval records in millis.\"\"\"\n",
        "  global global_last_time, global_time_list\n",
        "  if global_time_list:\n",
        "    return 1000 * global_time_list[-1]\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def dot(a, b):\n",
        "  \"\"\"Dot product function since TensorFlow doesn't have one.\"\"\"\n",
        "  return tf.reduce_sum(a*b)\n",
        "\n",
        "def verbose_func(s):\n",
        "  print(s)\n",
        "\n",
        "final_loss = None\n",
        "times = []\n",
        "def lbfgs(opfunc, x, config, state, do_verbose, log_fn):\n",
        "  \"\"\"port of lbfgs.lua, using TensorFlow eager mode.\n",
        "  \"\"\"\n",
        "\n",
        "  if config.maxIter == 0:\n",
        "    return\n",
        "\n",
        "  global final_loss, times\n",
        "  \n",
        "  maxIter = config.maxIter\n",
        "  maxEval = config.maxEval or maxIter*1.25\n",
        "  tolFun = config.tolFun or 1e-18\n",
        "  tolX = config.tolX or 1e-19\n",
        "  nCorrection = config.nCorrection or 100\n",
        "  lineSearch = config.lineSearch\n",
        "  lineSearchOpts = config.lineSearchOptions\n",
        "  learningRate = config.learningRate or 0.001\n",
        "  isverbose = config.verbose or False\n",
        "\n",
        "  # verbose function\n",
        "  if isverbose:\n",
        "    verbose = verbose_func\n",
        "  else:\n",
        "    verbose = lambda x: None\n",
        "\n",
        "    # evaluate initial f(x) and df/dx\n",
        "  f, g = opfunc(x)\n",
        "\n",
        "  f_hist = [f]\n",
        "  currentFuncEval = 1\n",
        "  state.funcEval = state.funcEval + 1\n",
        "  p = g.shape[0]\n",
        "\n",
        "  # check optimality of initial point\n",
        "  tmp1 = tf.abs(g)\n",
        "  if tf.reduce_sum(tmp1) <= tolFun:\n",
        "    verbose(\"optimality condition below tolFun\")\n",
        "    return x, f_hist\n",
        "\n",
        "  # optimize for a max of maxIter iterations\n",
        "  nIter = 0\n",
        "  times = []\n",
        "  while nIter < maxIter:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # keep track of nb of iterations\n",
        "    nIter = nIter + 1\n",
        "    state.nIter = state.nIter + 1\n",
        "\n",
        "    ############################################################\n",
        "    ## compute gradient descent direction\n",
        "    ############################################################\n",
        "    if state.nIter == 1:\n",
        "      d = -g\n",
        "      old_dirs = []\n",
        "      old_stps = []\n",
        "      Hdiag = 1\n",
        "    else:\n",
        "      # do lbfgs update (update memory)\n",
        "      y = g - g_old\n",
        "      s = d*t\n",
        "      ys = dot(y, s)\n",
        "      \n",
        "      if ys > 1e-10:\n",
        "        # updating memory\n",
        "        if len(old_dirs) == nCorrection:\n",
        "          # shift history by one (limited-memory)\n",
        "          del old_dirs[0]\n",
        "          del old_stps[0]\n",
        "\n",
        "        # store new direction/step\n",
        "        old_dirs.append(s)\n",
        "        old_stps.append(y)\n",
        "\n",
        "        # update scale of initial Hessian approximation\n",
        "        Hdiag = ys/dot(y, y)\n",
        "\n",
        "      # compute the approximate (L-BFGS) inverse Hessian \n",
        "      # multiplied by the gradient\n",
        "      k = len(old_dirs)\n",
        "\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      ro = [0]*nCorrection\n",
        "      for i in range(k):\n",
        "        ro[i] = 1/dot(old_stps[i], old_dirs[i])\n",
        "        \n",
        "\n",
        "      # iteration in L-BFGS loop collapsed to use just one buffer\n",
        "      # need to be accessed element-by-element, so don't re-type tensor:\n",
        "      al = [0]*nCorrection\n",
        "\n",
        "      q = -g\n",
        "      for i in range(k-1, -1, -1):\n",
        "        al[i] = dot(old_dirs[i], q) * ro[i]\n",
        "        q = q - al[i]*old_stps[i]\n",
        "\n",
        "      # multiply by initial Hessian\n",
        "      r = q*Hdiag\n",
        "      for i in range(k):\n",
        "        be_i = dot(old_stps[i], r) * ro[i]\n",
        "        r += (al[i]-be_i)*old_dirs[i]\n",
        "        \n",
        "      d = r\n",
        "      # final direction is in r/d (same object)\n",
        "\n",
        "    g_old = g\n",
        "    f_old = f\n",
        "    \n",
        "    ############################################################\n",
        "    ## compute step length\n",
        "    ############################################################\n",
        "    # directional derivative\n",
        "    gtd = dot(g, d)\n",
        "\n",
        "    # check that progress can be made along that direction\n",
        "    if gtd > -tolX:\n",
        "      verbose(\"Can not make progress along direction.\")\n",
        "      break\n",
        "\n",
        "    # reset initial guess for step size\n",
        "    if state.nIter == 1:\n",
        "      tmp1 = tf.abs(g)\n",
        "      t = min(1, 1/tf.reduce_sum(tmp1))\n",
        "    else:\n",
        "      t = learningRate\n",
        "\n",
        "\n",
        "    # optional line search: user function\n",
        "    lsFuncEval = 0\n",
        "    if lineSearch and isinstance(lineSearch) == types.FunctionType:\n",
        "      # perform line search, using user function\n",
        "      f,g,x,t,lsFuncEval = lineSearch(opfunc,x,t,d,f,g,gtd,lineSearchOpts)\n",
        "      f_hist.append(f)\n",
        "    else:\n",
        "      # no line search, simply move with fixed-step\n",
        "      x += t*d\n",
        "      \n",
        "      if nIter != maxIter:\n",
        "        # re-evaluate function only if not in last iteration\n",
        "        # the reason we do this: in a stochastic setting,\n",
        "        # no use to re-evaluate that function here\n",
        "        f, g = opfunc(x)\n",
        "        lsFuncEval = 1\n",
        "        f_hist.append(f)\n",
        "\n",
        "\n",
        "    # update func eval\n",
        "    currentFuncEval = currentFuncEval + lsFuncEval\n",
        "    state.funcEval = state.funcEval + lsFuncEval\n",
        "\n",
        "    ############################################################\n",
        "    ## check conditions\n",
        "    ############################################################\n",
        "    if nIter == maxIter:\n",
        "      break\n",
        "\n",
        "    if currentFuncEval >= maxEval:\n",
        "      # max nb of function evals\n",
        "      verbose('max nb of function evals')\n",
        "      break\n",
        "\n",
        "    tmp1 = tf.abs(g)\n",
        "    if tf.reduce_sum(tmp1) <=tolFun:\n",
        "      # check optimality\n",
        "      verbose('optimality condition below tolFun')\n",
        "      break\n",
        "    \n",
        "    tmp1 = tf.abs(d*t)\n",
        "    if tf.reduce_sum(tmp1) <= tolX:\n",
        "      # step size below tolX\n",
        "      verbose('step size below tolX')\n",
        "      break\n",
        "\n",
        "    if tf.abs(f-f_old) < tolX:\n",
        "      # function value changing less than tolX\n",
        "      verbose('function value changing less than tolX'+str(tf.abs(f-f_old)))\n",
        "      break\n",
        "\n",
        "    if do_verbose:\n",
        "      log_fn(nIter, f.numpy(), True)\n",
        "      #print(\"Step %3d loss %6.5f msec %6.3f\"%(nIter, f.numpy(), last_time()))\n",
        "      record_time()\n",
        "      times.append(last_time())\n",
        "\n",
        "    if nIter == maxIter - 1:\n",
        "      final_loss = f.numpy()\n",
        "\n",
        "\n",
        "  # save state\n",
        "  state.old_dirs = old_dirs\n",
        "  state.old_stps = old_stps\n",
        "  state.Hdiag = Hdiag\n",
        "  state.g_old = g_old\n",
        "  state.f_old = f_old\n",
        "  state.t = t\n",
        "  state.d = d\n",
        "\n",
        "  return x, f_hist, currentFuncEval\n",
        "\n",
        "# dummy/Struct gives Lua-like struct object with 0 defaults\n",
        "class dummy(object):\n",
        "  pass\n",
        "\n",
        "class Struct(dummy):\n",
        "  def __getattribute__(self, key):\n",
        "    if key == '__dict__':\n",
        "      return super(dummy, self).__getattribute__('__dict__')\n",
        "    return self.__dict__.get(key, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zVRSZXVxMTj"
      },
      "source": [
        "# Prep Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hEwFbF5UxLCC"
      },
      "outputs": [],
      "source": [
        "def prep_data(path, N_u=None, N_f=None):\n",
        "    # Reading external data [t is 100x1, usol is 256x100 (solution), x is 256x1]\n",
        "    #data = scipy.io.loadmat(path)\n",
        "    data = path\n",
        "    # Flatten makes [[]] into [], [:,None] makes it a column vector\n",
        "    t = data['t'].flatten()[:,None] # T x 1\n",
        "    x = data['x'].flatten()[:,None] # N x 1\n",
        "\n",
        "    # Keeping the 2D data for the solution data (real() is maybe to make it float by default, in case of zeroes)\n",
        "    Exact_u = np.real(data['usol']).T # T x N\n",
        "\n",
        "    # Meshing x and t in 2D (256,100)\n",
        "    X, T = np.meshgrid(x,t)\n",
        "\n",
        "    # Preparing the inputs x and t (meshed as X, T) for predictions in one single array, as X_star\n",
        "    X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
        "\n",
        "    # Preparing the testing u_star\n",
        "    u_star = Exact_u.flatten()[:,None]\n",
        "                \n",
        "    # Noiseless data TODO: add support for noisy data    \n",
        "    idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
        "    X_u_train = X_star[idx,:]\n",
        "    u_train = u_star[idx,:]\n",
        "\n",
        "    # Domain bounds (lowerbounds upperbounds) [x, t], which are here ([-1.0, 0.0] and [1.0, 1.0])\n",
        "    lb = X_star.min(axis=0)\n",
        "    ub = X_star.max(axis=0) \n",
        "    # Getting the initial conditions (t=0)\n",
        "    xx1 = np.hstack((X[0:1,:].T, T[0:1,:].T))\n",
        "    uu1 = Exact_u[0:1,:].T\n",
        "    # Getting the lowest boundary conditions (x=0) \n",
        "    xx2 = np.hstack((X[:,0:1], T[:,0:1]))\n",
        "    uu2 = Exact_u[:,0:1]\n",
        "\n",
        "    # Stacking them in multidimensional tensors for training (X_u_train is for now the continuous boundaries)\n",
        "    X_u_train = np.vstack([xx1, xx2])\n",
        "    u_train = np.vstack([uu1, uu2])\n",
        "\n",
        "    # Generating the x and t collocation points for f, with each having a N_f size\n",
        "    # We pointwise add and multiply to spread the LHS over the 2D domain\n",
        "    X_f_train = lb + (ub-lb)*lhs(2, N_f)\n",
        "\n",
        "    # Generating a uniform random sample from ints between 0, and the size of x_u_train, of size N_u (initial data size) and without replacement (unique)\n",
        "    idx = np.random.choice(X_u_train.shape[0], N_u, replace=False)\n",
        "    # Getting the corresponding X_u_train (which is now scarce boundary/initial coordinates)\n",
        "    X_u_train = X_u_train[idx,:]\n",
        "    # Getting the corresponding u_train\n",
        "    u_train = u_train [idx,:]\n",
        "\n",
        "    return x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, X_f_train, ub, lb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue5Wz7Dl2rBo"
      },
      "source": [
        "# HYPER PARAMETERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "hFnT-9oS2cDM"
      },
      "outputs": [],
      "source": [
        "# Data size on the solution u\n",
        "N_u = 300\n",
        "N_L = 5000\n",
        "N_i = 5000\n",
        "# Collocation points size, where we’ll check for f = 0\n",
        "N_f = 90000\n",
        "\n",
        "mode = 'M1' #Use M1 MLP, M2 for attention\n",
        "\n",
        "# Getting the data\n",
        "path = scipy.io.loadmat('/content/Buckley_Swc_0_Sor_0_M_2.mat');\n",
        "x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, X_f, ub, lb = prep_data(path, N_u, N_f)\n",
        "\n",
        "IC = lb + np.array([1.0, 0.0]) * lhs(2, N_i)\n",
        "Lb = lb + np.array([0.0, 1.0]) * lhs(2, N_L)\n",
        "S_bc_v = np.zeros((N_L, 1)) + 1\n",
        "S_ic_v = np.zeros((N_L, 1))\n",
        "\n",
        "# DeepNN topology (2-sized input [x t], 8 hidden layer of 20-width, 1-sized output [u]\n",
        "layers_part = [2, 20, 20, 20, 20, 1]\n",
        "layers_Dist = [2, 20, 20, 20, 20, 20, 1]\n",
        "layers = [2, 20, 20, 20, 20, 1]\n",
        "\n",
        "# Exponential Decaying learing rate\n",
        "initial_learning_rate = 1e-3\n",
        "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.96,\n",
        "    staircase=False)\n",
        "\n",
        "# Setting up the TF SGD-based optimizer (set tf_epochs=0 to cancel it)\n",
        "tf_epochs = 150000 \n",
        "tf_optimizer = tf.keras.optimizers.Adam(\n",
        "  learning_rate= lr_schedule,\n",
        "  beta_1=0.90,\n",
        "  epsilon=1e-7)\n",
        "\n",
        "# Setting up the quasi-newton LBGFS optimizer (set nt_epochs=0 to cancel it)\n",
        "nt_epochs = 0\n",
        "nt_config = Struct()\n",
        "nt_config.learningRate = 1e-2\n",
        "nt_config.maxIter = nt_epochs\n",
        "nt_config.nCorrection = 100\n",
        "nt_config.tolFun = 1.0*np.finfo(float).eps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANj1BCHoyoA9"
      },
      "source": [
        "# Physics Informed NN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "H5htyewH9NLa"
      },
      "outputs": [],
      "source": [
        "class PhysicsInformedNN(object):\n",
        "  def __init__(self, layers_part, layers_Dist, layers, optimizer, logger, X_f, IC, Lb, S_bc_v, S_ic_v, DIST, ub, lb, mode):\n",
        "    self.mode = mode\n",
        "    self.layers = layers\n",
        "\n",
        "    self.weights_S, self.biases_S = self.initialize_NN_S(layers)\n",
        "    \n",
        "    if mode in ['M2']:\n",
        "      # Initialize encoder weights and biases\n",
        "      self.encoder_weights_1_S = self.xavier_init([2, layers[1]])  \n",
        "      self.encoder_biases_1_S = self.xavier_init([1, layers[1]])\n",
        "\n",
        "      self.encoder_weights_2_S = self.xavier_init([2, layers[1]])\n",
        "      self.encoder_biases_2_S = self.xavier_init([1, layers[1]])\n",
        "\n",
        "    # Descriptive Keras model [2, 20, …, 20, 2] \n",
        "    self.Dist_model = tf.keras.Sequential()\n",
        "    self.Dist_model.add(tf.keras.layers.InputLayer(input_shape=(layers_Dist[0],)))\n",
        "    self.Dist_model.add(tf.keras.layers.Lambda(lambda X: 1.0*(X - lb)/(ub - lb) - 0.0))\n",
        "    for width in layers_Dist[1:-1]:\n",
        "        self.Dist_model.add(tf.keras.layers.Dense(width, activation=tf.nn.relu, kernel_initializer='glorot_normal'))\n",
        "    self.Dist_model.add(tf.keras.layers.Dense(layers_Dist[-1], activation=tf.nn.relu, kernel_initializer='glorot_normal')) \n",
        "\n",
        "    # Descriptive Keras model [2, 20, …, 20, 2]\n",
        "    self.part_model = tf.keras.Sequential()\n",
        "    self.part_model.add(tf.keras.layers.InputLayer(input_shape=(layers_part[0],)))\n",
        "    self.part_model.add(tf.keras.layers.Lambda(lambda X: X))\n",
        "    for width in layers_part[1:-1]:\n",
        "        self.part_model.add(tf.keras.layers.Dense(width, activation=tf.nn.tanh, kernel_initializer='glorot_normal'))\n",
        "    self.part_model.add(tf.keras.layers.Dense(layers_part[-1], activation=tf.nn.sigmoid, kernel_initializer='glorot_normal'))\n",
        "\n",
        "    # Computing the sizes of weights/biases for future decomposition\n",
        "    self.sizes_w = []\n",
        "    self.sizes_b = []\n",
        "    for i, width in enumerate(layers):\n",
        "      if i != 1:\n",
        "        self.sizes_w.append(int(width * layers[1]))\n",
        "        self.sizes_b.append(int(width if i != 0 else layers[1]))\n",
        "\n",
        "    # Computing the sizes of weights/biases for future decomposition - particular solution\n",
        "    self.sizes_w_part = []\n",
        "    self.sizes_b_part = []\n",
        "    for i, width in enumerate(layers_part):\n",
        "      if i != 1:\n",
        "        self.sizes_w_part.append(int(width * layers_part[1]))\n",
        "        self.sizes_b_part.append(int(width if i != 0 else layers_part[1]))\n",
        "\n",
        "    # Computing the sizes of weights/biases for future decomposition - particular solution\n",
        "    self.sizes_w_Dist = []\n",
        "    self.sizes_b_Dist = []\n",
        "    for i, width in enumerate(layers_Dist):\n",
        "      if i != 1:\n",
        "        self.sizes_w_Dist.append(int(width * layers_Dist[1]))\n",
        "        self.sizes_b_Dist.append(int(width if i != 0 else layers_Dist[1]))\n",
        "\n",
        "    self.optimizer = optimizer\n",
        "    self.logger = logger\n",
        "    self.dtype = tf.float32\n",
        "\n",
        "    # Separating the collocation coordinates\n",
        "    self.x_f = tf.convert_to_tensor(X_f[:, 0:1], dtype=self.dtype)\n",
        "    self.t_f = tf.convert_to_tensor(X_f[:, 1:2], dtype=self.dtype)\n",
        "\n",
        "  # Defining custom loss for particular solution network\n",
        "  @tf.function\n",
        "  def __loss_part(self, x_IC, t_IC, x_Lb, t_Lb, S_ic_v, S_bc_v):\n",
        "    S_i = self.part_model(tf.stack([x_IC, t_IC], axis=1))\n",
        "    S_bc = self.part_model(tf.stack([x_Lb, t_Lb], axis=1))    \n",
        "    return tf.reduce_mean(tf.square(S_i)) + tf.reduce_mean(tf.square(S_bc_v - S_bc))\n",
        "\n",
        "  # Defining custom loss for distance function network\n",
        "  @tf.function\n",
        "  def __loss_Dist(self, x_dist, t_dist, S_dist):\n",
        "    D_S = self.Dist_model(tf.stack([x_dist, t_dist], axis=1))\n",
        "    return tf.reduce_mean(tf.square(D_S - S_dist))\n",
        "    \n",
        "  # Defining custom loss\n",
        "  @tf.function\n",
        "  def __loss(self):\n",
        "    f_pred = self.f_model()\n",
        "    return tf.reduce_mean(tf.square(f_pred))\n",
        "\n",
        "  # Xavier initialization\n",
        "  def xavier_init(self, size):\n",
        "    in_dim = size[0]\n",
        "    out_dim = size[1]\n",
        "    xavier_stddev = 1. / np.sqrt((in_dim + out_dim) / 2.)\n",
        "    return tf.Variable(tf.random.normal([in_dim, out_dim], dtype=tf.float32) * xavier_stddev,\n",
        "                       dtype=tf.float32)\n",
        "    \n",
        "  # Initialize S network weights and biases using Xavier initialization\n",
        "  def initialize_NN_S(self, layers):\n",
        "    weights = []\n",
        "    biases = []\n",
        "    num_layers = len(layers)\n",
        "    for l in range(0, num_layers - 1):\n",
        "      W = self.xavier_init(size=[layers[l], layers[l + 1]])\n",
        "      b = tf.Variable(tf.zeros([1, layers[l + 1]], dtype=tf.float32), dtype=tf.float32)\n",
        "      weights.append(W)\n",
        "      biases.append(b)\n",
        "    return weights, biases\n",
        "\n",
        "  # Evaluates the forward pass.\n",
        "  @tf.function\n",
        "  def forward_pass_S(self, H):\n",
        "    H = 1.0*(H - lb)/(ub - lb) - 0.0\n",
        "    if self.mode in ['M1']:\n",
        "      num_layers = len(self.layers)\n",
        "      for l in range(0, num_layers - 2):\n",
        "        W = self.weights_S[l]\n",
        "        b = self.biases_S[l]\n",
        "        H = tf.tanh(tf.add(tf.matmul(H, W), b))\n",
        "      W = self.weights_S[-1]\n",
        "      b = self.biases_S[-1]\n",
        "      H = tf.add(tf.matmul(H, W), b)\n",
        "      return H\n",
        "    if self.mode in ['M2']:\n",
        "      num_layers = len(self.layers)\n",
        "      encoder_1 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_1_S), self.encoder_biases_1_S))\n",
        "      encoder_2 = tf.tanh(tf.add(tf.matmul(H, self.encoder_weights_2_S), self.encoder_biases_2_S))\n",
        "      for l in range(0, num_layers - 2):\n",
        "        W = self.weights_S[l]\n",
        "        b = self.biases_S[l]\n",
        "        H = tf.math.multiply(tf.tanh(tf.add(tf.matmul(H, W), b)), encoder_1) + \\\n",
        "        tf.math.multiply(tf.tanh(tf.add(1 - tf.matmul(H, W), b)), encoder_2)\n",
        "      W = self.weights_S[-1]\n",
        "      b = self.biases_S[-1]\n",
        "      H = tf.add(tf.matmul(H, W), b)\n",
        "      return H\n",
        "\n",
        "  @tf.function\n",
        "  def net_S(self, x, t):\n",
        "    u = self.forward_pass_S(tf.stack([x[:,0], t[:,0]], axis=1))\n",
        "    part = self.part_model(tf.stack([x[:,0], t[:,0]], axis=1))\n",
        "    dist = self.Dist_model(tf.stack([x[:,0], t[:,0]], axis=1))\n",
        "    S = part + (dist * u)\n",
        "    return S\n",
        "\n",
        "  # The actual PINN\n",
        "  @tf.function\n",
        "  def f_model(self):    \n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "      tape.watch(self.x_f)\n",
        "      tape.watch(self.t_f)\n",
        "      S = self.net_S(self.x_f, self.t_f)\n",
        "\n",
        "      Swc = 0.0\n",
        "      M = 2\n",
        "      Sor = 0.0\n",
        "      #Non-Convex Flux Function\n",
        "      frac_org = tf.divide(tf.square(S-Swc), tf.square(S-Swc) + tf.divide(tf.square(1 - S - Sor), M))\n",
        "      Sf = tf.sqrt(tf.divide(1/M, (1/M)+1))\n",
        "      frac_Sf = tf.divide(tf.square(Sf-Swc), tf.square(Sf-Swc) + tf.divide(tf.square(1 - Sf - Sor), M))\n",
        "      frac = tf.divide(frac_Sf, Sf)*S - tf.multiply(tf.divide(frac_Sf, Sf)*S, tnp.heaviside(S-Sf, 1)) + tf.multiply(frac_org, tnp.heaviside(S-Sf, 1)) \n",
        "    S_x = tape.gradient(S, self.x_f)\n",
        "    S_t = tape.gradient(S, self.t_f)\n",
        "    frac_S = tape.gradient(frac, S)\n",
        "    del tape \n",
        "    return S_t + tf.multiply(frac_S, S_x)\n",
        "\n",
        "  def get_weights_model(self, model):\n",
        "    w = []\n",
        "    for layer in model.layers[1:]:\n",
        "      weights_biases = layer.get_weights()\n",
        "      weights = weights_biases[0].flatten()\n",
        "      biases = weights_biases[1]\n",
        "      w.extend(weights)\n",
        "      w.extend(biases)\n",
        "    return tf.convert_to_tensor(w, dtype=self.dtype)\n",
        "\n",
        "  def get_weights(self):\n",
        "    w_S = []\n",
        "    for i in range(len(self.weights_S)):\n",
        "      W_S = self.weights_S[i]\n",
        "      b_S = self.biases_S[i]\n",
        "      w_S.append(W_S)\n",
        "      w_S.append(b_S)\n",
        "    return w_S\n",
        "\n",
        "  def get_weights_lbfgs(self):\n",
        "    self.w_S = []\n",
        "    for i in range(len(self.weights_S)):\n",
        "      W_S = np.array(self.weights_S[i]).flatten()\n",
        "      b_S = np.array(self.biases_S[i]).flatten()\n",
        "      self.w_S.extend(W_S)\n",
        "      self.w_S.extend(b_S)\n",
        "    w = self.w_S\n",
        "    return tf.convert_to_tensor(w, dtype=tf.float32)\n",
        "\n",
        "  def set_weights(self, w):\n",
        "    w_S = w[0:len(self.w_S)]\n",
        "    weights1_S = []\n",
        "    biases1_S = []\n",
        "    for i, q in enumerate(self.layers[1:]):\n",
        "      start_weights = sum(self.sizes_w[:i]) + sum(self.sizes_b[:i])\n",
        "      end_weights = sum(self.sizes_w[:i+1]) + sum(self.sizes_b[:i])\n",
        "      weights_S = w_S[start_weights:end_weights]\n",
        "      w_div = int(self.sizes_w[i] / self.sizes_b[i])\n",
        "      weights_S = tf.reshape(weights_S, [w_div, self.sizes_b[i]])\n",
        "      biases_S = w_S[end_weights:end_weights + self.sizes_b[i]]\n",
        "      biases_S = tf.reshape(biases_S, [1, self.sizes_b[i]])  \n",
        "      weights1_S.append(tf.Variable(weights_S))\n",
        "      biases1_S.append(tf.Variable(biases_S))\n",
        "      if np.array(weights1_S).shape == ((len(layers)-1),):\n",
        "        self.weights_S = weights1_S\n",
        "        self.biases_S = biases1_S\n",
        "\n",
        "  def set_weights_part(self, w):\n",
        "    for i, layer in enumerate(self.part_model.layers[1:]):\n",
        "      start_weights = sum(self.sizes_w_part[:i]) + sum(self.sizes_b_part[:i])\n",
        "      end_weights = sum(self.sizes_w_part[:i+1]) + sum(self.sizes_b_part[:i])\n",
        "      weights = w[start_weights:end_weights]\n",
        "      w_div = int(self.sizes_w_part[i] / self.sizes_b_part[i])\n",
        "      weights = tf.reshape(weights, [w_div, self.sizes_b_part[i]])\n",
        "      biases = w[end_weights:end_weights + self.sizes_b_part[i]]\n",
        "      weights_biases = [weights, biases]\n",
        "      layer.set_weights(weights_biases)\n",
        "\n",
        "  def set_weights_Dist(self, w):\n",
        "    for i, layer in enumerate(self.Dist_model.layers[1:]):\n",
        "      start_weights = sum(self.sizes_w_Dist[:i]) + sum(self.sizes_b_Dist[:i])\n",
        "      end_weights = sum(self.sizes_w_Dist[:i+1]) + sum(self.sizes_b_Dist[:i])\n",
        "      weights = w[start_weights:end_weights]\n",
        "      w_div = int(self.sizes_w_Dist[i] / self.sizes_b_Dist[i])\n",
        "      weights = tf.reshape(weights, [w_div, self.sizes_b_Dist[i]])\n",
        "      biases = w[end_weights:end_weights + self.sizes_b_Dist[i]]\n",
        "      weights_biases = [weights, biases]\n",
        "      layer.set_weights(weights_biases)\n",
        "\n",
        "  # The training function\n",
        "  def fit_part(self, tf_epochs, nt_config):\n",
        "    self.logger.log_train_start(self)\n",
        "\n",
        "    #Left Boundary \n",
        "    self.x_Lb = tf.convert_to_tensor(Lb[:, 0:1], dtype=self.dtype)\n",
        "    self.t_Lb = tf.convert_to_tensor(Lb[:, 1:2], dtype=self.dtype)\n",
        "    self.S_bc_v = tf.convert_to_tensor(S_bc_v, dtype=self.dtype)\n",
        "\n",
        "    # Initial condition point, t=0\n",
        "    self.x_IC = tf.convert_to_tensor(IC[:, 0:1], dtype=self.dtype)\n",
        "    self.t_IC = tf.convert_to_tensor(IC[:, 1:2], dtype=self.dtype)\n",
        "    self.S_ic_v = tf.convert_to_tensor(S_ic_v, dtype=self.dtype)\n",
        "\n",
        "    self.logger.log_train_opt(\"Adam\")\n",
        "    for epoch in range(tf_epochs):\n",
        "      # Optimization step\n",
        "      with tf.GradientTape() as tape:\n",
        "        loss_part = self.__loss_part(self.x_IC, self.t_IC, self.x_Lb, self.t_Lb, self.S_ic_v, self.S_bc_v)\n",
        "      grad = tape.gradient(loss_part, self.part_model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grad, self.part_model.trainable_variables))\n",
        "      self.logger.log_train_epoch(epoch, loss_part)\n",
        "\n",
        "    self.logger.log_train_opt(\"LBFGS\")\n",
        "    def loss_and_flat_grad(w):\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.set_weights_part(w)\n",
        "        loss_part = self.__loss_part(self.x_IC, self.t_IC, self.x_Lb, self.t_Lb, self.S_ic_v, self.S_bc_v)\n",
        "      grad = tape.gradient(loss_part, self.part_model.trainable_variables)\n",
        "      \n",
        "      grad_flat = []\n",
        "      for g in grad:\n",
        "        grad_flat.append(tf.reshape(g, [-1]))\n",
        "      grad_flat =  tf.concat(grad_flat, 0)\n",
        "      return loss_part, grad_flat\n",
        "\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "      self.get_weights_model(self.part_model),\n",
        "      nt_config, Struct(), True,\n",
        "      lambda epoch, loss, is_iter:\n",
        "        self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
        "\n",
        "  # The training function\n",
        "  def fit_Dist(self, tf_epochs, nt_config):\n",
        "    self.logger.log_train_start(self)\n",
        "\n",
        "    # Distance function\n",
        "    self.x_dist = tf.convert_to_tensor(DIST[:, 0:1], dtype=self.dtype)\n",
        "    self.t_dist = tf.convert_to_tensor(DIST[:, 1:2], dtype=self.dtype)\n",
        "    self.S_dist = tf.convert_to_tensor(DIST[:, 2:3], dtype=self.dtype)\n",
        "\n",
        "    self.logger.log_train_opt(\"Adam\")\n",
        "    for epoch in range(tf_epochs):\n",
        "      # Optimization step\n",
        "      with tf.GradientTape() as tape:\n",
        "        loss_Dist = self.__loss_Dist(self.x_dist, self.t_dist, self.S_dist)\n",
        "      grad = tape.gradient(loss_Dist, self.Dist_model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grad, self.Dist_model.trainable_variables))\n",
        "      self.logger.log_train_epoch(epoch, loss_Dist)\n",
        "\n",
        "    self.logger.log_train_opt(\"LBFGS\")\n",
        "    def loss_and_flat_grad(w):\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.set_weights_Dist(w)\n",
        "        loss_Dist = self.__loss_Dist(self.x_dist, self.t_dist, self.S_dist)\n",
        "      grad = tape.gradient(loss_Dist, self.Dist_model.trainable_variables)\n",
        "      \n",
        "      grad_flat = []\n",
        "      for g in grad:\n",
        "        grad_flat.append(tf.reshape(g, [-1]))\n",
        "      grad_flat =  tf.concat(grad_flat, 0)\n",
        "      return loss_Dist, grad_flat\n",
        "\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "      self.get_weights_model(self.Dist_model),\n",
        "      nt_config, Struct(), True,\n",
        "      lambda epoch, loss, is_iter:\n",
        "        self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
        "\n",
        "  # The training function\n",
        "  def fit(self, tf_epochs, nt_config=Struct()):\n",
        "    self.logger.log_train_start(self)\n",
        "\n",
        "    self.logger.log_train_opt(\"Adam\")\n",
        "    for epoch in range(tf_epochs):\n",
        "      # Optimization step\n",
        "      with tf.GradientTape() as tape:\n",
        "        loss_value = self.__loss()\n",
        "      grad = tape.gradient(loss_value, self.get_weights())\n",
        "      self.optimizer.apply_gradients(zip(grad, self.get_weights()))\n",
        "      self.logger.log_train_epoch(epoch, loss_value)\n",
        "\n",
        "    self.logger.log_train_opt(\"LBFGS\")\n",
        "    def loss_and_flat_grad(w):\n",
        "      with tf.GradientTape() as tape:\n",
        "        self.set_weights(w)\n",
        "        loss_value = self.__loss()\n",
        "      grad = tape.gradient(loss_value, self.get_weights())\n",
        "      grad_flat = []\n",
        "      for g in grad:\n",
        "        grad_flat.append(tf.reshape(g, [-1]))\n",
        "      grad_flat =  tf.concat(grad_flat, 0)\n",
        "      return loss_value, grad_flat\n",
        "\n",
        "    lbfgs(loss_and_flat_grad,\n",
        "      self.get_weights_lbfgs(),\n",
        "      nt_config, Struct(), True,\n",
        "      lambda epoch, loss, is_iter:\n",
        "        self.logger.log_train_epoch(epoch, loss, \"\", is_iter))\n",
        "    \n",
        "    self.logger.log_train_end(tf_epochs + nt_config.maxIter)\n",
        "\n",
        "  def predict(self, X_star):\n",
        "    x = tf.convert_to_tensor(X_star[:, 0:1], dtype=self.dtype)\n",
        "    t = tf.convert_to_tensor(X_star[:, 1:2], dtype=self.dtype)\n",
        "    S = self.net_S(x, t)\n",
        "    return S\n",
        "\n",
        "  def predict_part(self, X_star):\n",
        "    x_p = tf.convert_to_tensor(X_star[:, 0:1], dtype=self.dtype)\n",
        "    t_p = tf.convert_to_tensor(X_star[:, 1:2], dtype=self.dtype)\n",
        "    Part_S = self.part_model(tf.stack([x_p, t_p], axis=1))\n",
        "    return Part_S\n",
        "\n",
        "  def predict_Dist(self, X_star):\n",
        "    x_D = tf.convert_to_tensor(X_star[:, 0:1], dtype=self.dtype)\n",
        "    t_D = tf.convert_to_tensor(X_star[:, 1:2], dtype=self.dtype)\n",
        "    D_S = self.Dist_model(tf.stack([x_D, t_D], axis=1))\n",
        "    return D_S\n",
        "\n",
        "def GenDistPt(xmin, xmax, tmin, tmax, num):\n",
        "    # num: number per edge\n",
        "    x = np.linspace(xmin, xmax, num=num)\n",
        "    t = np.linspace(tmin, tmax, num=num)\n",
        "    \n",
        "    xxx, ttt = np.meshgrid(x, t)\n",
        "    xxx = xxx.flatten()[:, None]\n",
        "    ttt = ttt.flatten()[:, None]\n",
        "    return xxx, ttt\n",
        "\n",
        "def GenDist(XYT_dist):\n",
        "    dist_S = np.zeros_like(XYT_dist[:, 0:1])\n",
        "    for i in range(len(XYT_dist)):\n",
        "        dist_S[i, 0] = min(XYT_dist[i][0], XYT_dist[i][1])  # Initial Condition & Left Boundary \n",
        "    DIST = np.concatenate((XYT_dist, dist_S), 1)\n",
        "    idx = np.random.choice(DIST.shape[0], DIST.shape[0], replace=False)\n",
        "    DIST = DIST[idx,:]\n",
        "    return DIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "G7yEBCcl8M0f"
      },
      "outputs": [],
      "source": [
        "# Generate distance function for spatio-temporal space\n",
        "x_dist, t_dist = GenDistPt(xmin=0, xmax=1.0, tmin=0, tmax=1.0, num=50)\n",
        "XYT_dist = np.concatenate((x_dist, t_dist), 1)\n",
        "DIST = GenDist(XYT_dist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjeaUYPD4WiQ"
      },
      "source": [
        "# Training - Preprocessing - Distance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sghZ8DIR5F6V"
      },
      "outputs": [],
      "source": [
        "# Training the Distance function NN\n",
        "logger = Logger(frequency=100)\n",
        "pinn = PhysicsInformedNN(layers_part, layers_Dist, layers, tf_optimizer, logger, X_f, IC, Lb, S_bc_v, S_ic_v, DIST, ub, lb, mode)\n",
        "pinn.fit_Dist(tf_epochs, nt_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7sXoEbaw4dsp"
      },
      "outputs": [],
      "source": [
        "# Saving Distance Model Parameters\n",
        "Dist_Model = pinn.Dist_model\n",
        "Dist_Model.save_weights('Distance_model_weights_BL.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQYEe0T_vbwv"
      },
      "source": [
        "# Training - Preprocessing - *Particular*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fJLu6ZjHvhox"
      },
      "outputs": [],
      "source": [
        "# Training the Particular Solution NN\n",
        "logger = Logger(frequency=100)\n",
        "pinn = PhysicsInformedNN(layers_part, layers_Dist, layers, tf_optimizer, logger, X_f, IC, Lb, S_bc_v, S_ic_v, DIST, ub, lb, mode)\n",
        "pinn.fit_part(tf_epochs, nt_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwNbY1RUvhU5"
      },
      "outputs": [],
      "source": [
        "# Saving Particular Model Parameters\n",
        "Part_model = pinn.part_model\n",
        "Part_model.save_weights('Particular_model_weights_BL.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIZNI8nc-9qv"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8e4VOHq2hF7"
      },
      "outputs": [],
      "source": [
        "# Getting the data\n",
        "path = scipy.io.loadmat('/content/Buckley_Swc_0_Sor_0_M_2.mat');\n",
        "x, t, X, T, Exact_u, X_star, u_star, X_u_train, u_train, X_f, ub, lb = prep_data(path, N_u, N_f)\n",
        "\n",
        "# Creating the model and training\n",
        "logger = Logger(frequency=200)\n",
        "pinn = PhysicsInformedNN(layers_part, layers_Dist, layers, tf_optimizer, logger, X_f, IC, Lb, S_bc_v, S_ic_v, DIST, ub, lb, mode)\n",
        "\n",
        "# Loading Particular Model Parameters\n",
        "Part_model = pinn.part_model\n",
        "Part_model.load_weights('Particular_model_weights_BL.h5')\n",
        "\n",
        "# Loading Distance Model Parameters\n",
        "Dist_Model = pinn.Dist_model\n",
        "pinn.Dist_model.load_weights('Distance_model_weights_BL.h5')\n",
        "\n",
        "pinn.fit(tf_epochs, nt_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cl7LvVgIA2iZ"
      },
      "outputs": [],
      "source": [
        "# L2 Error\n",
        "u_pred = pinn.predict(X_star)\n",
        "print('L2 Error: %.3e' %(np.linalg.norm(u_star - u_pred) / np.linalg.norm(u_star)))\n",
        "\n",
        "# Plotting\n",
        "plot_inf_cont_results(X_star, u_pred.numpy().flatten(), X_u_train, u_train, Exact_u, X, T, x, t)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "background_execution": "on",
      "collapsed_sections": [
        "sjvyrMzvyxWK",
        "4zVRSZXVxMTj",
        "Ue5Wz7Dl2rBo",
        "ANj1BCHoyoA9",
        "MjeaUYPD4WiQ",
        "wQYEe0T_vbwv"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1TPtVWtdfhCfuJZloHVNrfFCd2Ur_YxCZ",
      "authorship_tag": "ABX9TyPCZWd3KrGjj2NvlzeEW0wY",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}